#!/usr/bin/env python3
"""
Santiago Automated Documentation Service

An intelligent documentation automation system that keeps documentation
up-to-date and integrates with kanban workflow steps. This service:

- Auto-generates API documentation from code analysis
- Updates README files and project documentation
- Triggers documentation updates when features move through kanban columns
- Maintains cross-references between code and documentation
- Provides documentation quality metrics and suggestions

The service integrates with the kanban workflow to automatically update
documentation at key workflow transitions (ready â†’ in_progress, in_progress â†’ review, etc.)
"""

import asyncio
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from datetime import datetime
import ast
import inspect
import importlib.util

from santiago_core.core.mcp_service import MCPServer, MCPTool, MCPToolResult
from santiago_core.services.kanban_service import SantiagoKanbanService
from santiago_core.services.knowledge_graph import SantiagoKnowledgeGraph


class DocumentationAutomationService(MCPServer):
    """Automated documentation service for Santiago Factory"""

    def __init__(self, workspace_path: Path):
        super().__init__(
            name="santiago-docs-automation",
            version="1.0.0",
            description="Automated documentation system that keeps docs synchronized with code and workflow"
        )

        self.workspace_path = workspace_path
        self.knowledge_graph = SantiagoKnowledgeGraph(workspace_path)
        self.kanban_service = SantiagoKanbanService(workspace_path)

        # Documentation templates and patterns
        self.templates = self._load_templates()
        self.doc_patterns = self._load_doc_patterns()

        # Register tools
        self.register_tools()

    def _load_templates(self) -> Dict[str, str]:
        """Load documentation templates"""
        return {
            'api_endpoint': """### `{method} {path}`

**Description:** {description}

**Parameters:**
{parameters}

**Request Body:**
```json
{request_example}
```

**Response:**
```json
{response_example}
```

**Status Codes:**
{status_codes}

**Authentication:** {auth_required}
""",

            'python_class': """## Class: `{class_name}`

**Location:** `{file_path}:{line_number}`

**Description:** {description}

**Inheritance:** {inheritance}

**Public Methods:**
{methods}

**Usage Example:**
```python
{usage_example}
```
""",

            'python_function': """### Function: `{function_name}`

**Location:** `{file_path}:{line_number}`

**Signature:** `{signature}`

**Description:** {description}

**Parameters:**
{parameters}

**Returns:** {returns}

**Raises:** {raises}
""",

            'readme_update': """# {project_name}

{description}

## ðŸš€ Quick Start

{quick_start}

## ðŸ“š Documentation

{doc_links}

## ðŸ”§ Development

{dev_setup}

## ðŸ“Š Status

{status_badges}

---

**Last updated:** {timestamp}
**Auto-generated by:** Santiago Documentation Automation
"""
        }

    def _load_doc_patterns(self) -> Dict[str, re.Pattern]:
        """Load regex patterns for documentation extraction"""
        return {
            'docstring': re.compile(r'""".*?"""', re.DOTALL),
            'function_def': re.compile(r'def\s+(\w+)\s*\([^)]*\):'),
            'class_def': re.compile(r'class\s+(\w+)(\([^)]*\))?:'),
            'api_route': re.compile(r'@(?:app|router|api)\.(?:route|get|post|put|delete|patch)\s*\(\s*[\'"]([^\'"]*)[\'"]'),
            'todo_comment': re.compile(r'#\s*TODO:\s*(.+)', re.IGNORECASE),
            'fixme_comment': re.compile(r'#\s*FIXME:\s*(.+)', re.IGNORECASE),
        }

    def register_tools(self):
        """Register all documentation automation MCP tools"""

        # Core documentation generation tools
        self.register_tool(MCPTool(
            name="docs_generate_api_docs",
            description="Auto-generate API documentation from code analysis",
            parameters={
                "source_path": {"type": "string", "description": "Path to source code directory"},
                "output_path": {"type": "string", "description": "Path for generated API docs"},
                "include_private": {"type": "boolean", "description": "Include private methods/functions", "required": False, "default": False}
            }
        ))

        self.register_tool(MCPTool(
            name="docs_update_readme",
            description="Update README.md with current project status and documentation links",
            parameters={
                "readme_path": {"type": "string", "description": "Path to README file", "required": False, "default": "README.md"},
                "project_info": {"type": "object", "description": "Project information override", "required": False}
            }
        ))

        self.register_tool(MCPTool(
            name="docs_scan_codebase",
            description="Scan codebase for documentation gaps and generate improvement suggestions",
            parameters={
                "source_paths": {"type": "array", "description": "Paths to scan for documentation", "items": {"type": "string"}},
                "output_report": {"type": "string", "description": "Path for documentation audit report", "required": False}
            }
        ))

        # Kanban workflow integration tools
        self.register_tool(MCPTool(
            name="docs_on_kanban_transition",
            description="Trigger documentation updates when kanban cards transition between columns",
            parameters={
                "board_id": {"type": "string", "description": "Kanban board ID"},
                "card_id": {"type": "string", "description": "Card that transitioned"},
                "from_column": {"type": "string", "description": "Previous column"},
                "to_column": {"type": "string", "description": "New column"},
                "transition_type": {"type": "string", "description": "Type of transition (feature_start, feature_complete, etc.)"}
            }
        ))

        self.register_tool(MCPTool(
            name="docs_generate_feature_docs",
            description="Generate comprehensive documentation for a completed feature",
            parameters={
                "feature_id": {"type": "string", "description": "Feature/item identifier"},
                "implementation_files": {"type": "array", "description": "Files that implement the feature", "items": {"type": "string"}},
                "output_dir": {"type": "string", "description": "Directory for feature documentation", "required": False, "default": "docs/features"}
            }
        ))

        # Documentation maintenance tools
        self.register_tool(MCPTool(
            name="docs_validate_links",
            description="Validate all documentation links and cross-references",
            parameters={
                "docs_path": {"type": "string", "description": "Path to documentation directory", "required": False, "default": "docs"},
                "fix_broken": {"type": "boolean", "description": "Automatically fix broken links", "required": False, "default": False}
            }
        ))

        self.register_tool(MCPTool(
            name="docs_sync_with_code",
            description="Synchronize documentation with recent code changes",
            parameters={
                "changed_files": {"type": "array", "description": "Files that have changed", "items": {"type": "string"}},
                "docs_path": {"type": "string", "description": "Documentation directory", "required": False, "default": "docs"}
            }
        ))

        # Quality and metrics tools
        self.register_tool(MCPTool(
            name="docs_quality_metrics",
            description="Generate documentation quality metrics and coverage report",
            parameters={
                "docs_path": {"type": "string", "description": "Documentation directory", "required": False, "default": "docs"},
                "source_path": {"type": "string", "description": "Source code directory", "required": False, "default": "src"}
            }
        ))

    async def handle_tool_call(self, tool_name: str, parameters: Dict[str, Any]) -> MCPToolResult:
        """Handle MCP tool calls for documentation automation"""

        try:
            if tool_name == "docs_generate_api_docs":
                return await self._handle_generate_api_docs(parameters)
            elif tool_name == "docs_update_readme":
                return await self._handle_update_readme(parameters)
            elif tool_name == "docs_scan_codebase":
                return await self._handle_scan_codebase(parameters)
            elif tool_name == "docs_on_kanban_transition":
                return await self._handle_kanban_transition(parameters)
            elif tool_name == "docs_generate_feature_docs":
                return await self._handle_generate_feature_docs(parameters)
            elif tool_name == "docs_validate_links":
                return await self._handle_validate_links(parameters)
            elif tool_name == "docs_sync_with_code":
                return await self._handle_sync_with_code(parameters)
            elif tool_name == "docs_quality_metrics":
                return await self._handle_quality_metrics(parameters)
            else:
                return MCPToolResult(error=f"Unknown tool: {tool_name}")

        except Exception as e:
            return MCPToolResult(error=f"Error executing {tool_name}: {str(e)}")

    async def _handle_generate_api_docs(self, params: Dict[str, Any]) -> MCPToolResult:
        """Generate API documentation from code analysis"""
        source_path = Path(params["source_path"])
        output_path = Path(params["output_path"])
        include_private = params.get("include_private", False)

        # Analyze source code
        api_docs = await self._analyze_api_code(source_path, include_private)

        # Generate documentation
        docs_content = self._generate_api_docs_content(api_docs)

        # Write to output file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(docs_content)

        return MCPToolResult(result={
            "api_docs_generated": str(output_path),
            "endpoints_found": len(api_docs.get("endpoints", [])),
            "classes_found": len(api_docs.get("classes", [])),
            "functions_found": len(api_docs.get("functions", []))
        })

    async def _handle_update_readme(self, params: Dict[str, Any]) -> MCPToolResult:
        """Update README with current project status"""
        readme_path = Path(params.get("readme_path", "README.md"))
        project_info = params.get("project_info", {})

        # Gather project information
        project_data = await self._gather_project_info()
        project_data.update(project_info)

        # Generate README content
        readme_content = self.templates["readme_update"].format(**project_data)

        # Update README
        readme_path.write_text(readme_content)

        return MCPToolResult(result={
            "readme_updated": str(readme_path),
            "project_name": project_data.get("project_name", "Unknown"),
            "last_updated": project_data.get("timestamp")
        })

    async def _handle_scan_codebase(self, params: Dict[str, Any]) -> MCPToolResult:
        """Scan codebase for documentation gaps"""
        source_paths = [Path(p) for p in params["source_paths"]]
        output_report = params.get("output_report")

        # Scan for documentation issues
        issues = await self._scan_documentation_issues(source_paths)

        # Generate report
        report_content = self._generate_audit_report(issues)

        if output_report:
            Path(output_report).write_text(report_content)

        return MCPToolResult(result={
            "issues_found": len(issues),
            "report_generated": output_report or "in_memory",
            "critical_issues": len([i for i in issues if i["severity"] == "critical"]),
            "suggestions": [i["suggestion"] for i in issues[:5]]  # Top 5 suggestions
        })

    async def _handle_kanban_transition(self, params: Dict[str, Any]) -> MCPToolResult:
        """Handle kanban workflow transitions for documentation"""
        board_id = params["board_id"]
        card_id = params["card_id"]
        from_column = params["from_column"]
        to_column = params["to_column"]
        transition_type = params["transition_type"]

        # Determine what documentation actions to take based on transition
        actions_taken = []

        if transition_type == "feature_start":
            # Generate initial documentation stub
            actions_taken.append(await self._generate_feature_stub(board_id, card_id))

        elif transition_type == "feature_complete":
            # Generate comprehensive feature documentation
            actions_taken.append(await self._generate_complete_feature_docs(board_id, card_id))

        elif transition_type == "code_review":
            # Update API docs if needed
            actions_taken.append(await self._update_api_docs_on_review(board_id, card_id))

        # Update kanban card with documentation status
        await self._update_card_docs_status(board_id, card_id, actions_taken)

        return MCPToolResult(result={
            "transition_processed": f"{from_column} â†’ {to_column}",
            "actions_taken": len(actions_taken),
            "documentation_updated": True
        })

    async def _handle_generate_feature_docs(self, params: Dict[str, Any]) -> MCPToolResult:
        """Generate comprehensive feature documentation"""
        feature_id = params["feature_id"]
        implementation_files = [Path(f) for f in params["implementation_files"]]
        output_dir = Path(params.get("output_dir", "docs/features"))

        # Analyze implementation
        feature_analysis = await self._analyze_feature_implementation(implementation_files)

        # Generate documentation
        docs_content = self._generate_feature_docs_content(feature_id, feature_analysis)

        # Write to file
        output_dir.mkdir(parents=True, exist_ok=True)
        docs_file = output_dir / f"{feature_id}.md"
        docs_file.write_text(docs_content)

        return MCPToolResult(result={
            "feature_docs_generated": str(docs_file),
            "files_analyzed": len(implementation_files),
            "documentation_sections": len(feature_analysis.get("sections", []))
        })

    async def _handle_validate_links(self, params: Dict[str, Any]) -> MCPToolResult:
        """Validate documentation links and cross-references"""
        docs_path = Path(params.get("docs_path", "docs"))
        fix_broken = params.get("fix_broken", False)

        # Find all documentation files
        doc_files = list(docs_path.rglob("*.md")) + list(docs_path.rglob("*.rst"))

        # Validate links
        validation_results = await self._validate_doc_links(doc_files)

        if fix_broken:
            # Attempt to fix broken links
            fixes_applied = await self._fix_broken_links(validation_results["broken_links"])
            validation_results["fixes_applied"] = fixes_applied

        return MCPToolResult(result={
            "files_checked": len(doc_files),
            "broken_links": len(validation_results.get("broken_links", [])),
            "valid_links": validation_results.get("valid_links", 0),
            "fixes_applied": validation_results.get("fixes_applied", 0) if fix_broken else 0
        })

    async def _handle_sync_with_code(self, params: Dict[str, Any]) -> MCPToolResult:
        """Synchronize documentation with code changes"""
        changed_files = [Path(f) for f in params["changed_files"]]
        docs_path = Path(params.get("docs_path", "docs"))

        # Analyze what changed
        changes = await self._analyze_code_changes(changed_files)

        # Update relevant documentation
        updates_made = await self._update_docs_for_changes(changes, docs_path)

        return MCPToolResult(result={
            "files_changed": len(changed_files),
            "documentation_updates": len(updates_made),
            "sections_updated": sum(len(update.get("sections", [])) for update in updates_made)
        })

    async def _handle_quality_metrics(self, params: Dict[str, Any]) -> MCPToolResult:
        """Generate documentation quality metrics"""
        docs_path = Path(params.get("docs_path", "docs"))
        source_path = Path(params.get("source_path", "src"))

        # Calculate metrics
        metrics = await self._calculate_docs_quality_metrics(docs_path, source_path)

        return MCPToolResult(result={
            "documentation_coverage": metrics.get("coverage_percentage", 0),
            "total_doc_files": metrics.get("doc_files_count", 0),
            "code_files_with_docs": metrics.get("code_files_with_docs", 0),
            "quality_score": metrics.get("overall_quality_score", 0),
            "issues_found": len(metrics.get("quality_issues", []))
        })

    # Core documentation generation methods

    async def _analyze_api_code(self, source_path: Path, include_private: bool = False) -> Dict[str, Any]:
        """Analyze source code to extract API information"""
        api_info = {
            "endpoints": [],
            "classes": [],
            "functions": []
        }

        # Find Python files
        python_files = list(source_path.rglob("*.py"))

        for file_path in python_files:
            try:
                # Parse the file
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)

                # Extract classes and functions
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        if not include_private and node.name.startswith('_'):
                            continue

                        class_info = self._extract_class_info(node, file_path, content)
                        api_info["classes"].append(class_info)

                    elif isinstance(node, ast.FunctionDef):
                        if not include_private and node.name.startswith('_'):
                            continue

                        func_info = self._extract_function_info(node, file_path, content)
                        api_info["functions"].append(func_info)

                # Look for API route decorators
                for match in self.doc_patterns['api_route'].finditer(content):
                    endpoint_info = self._extract_endpoint_info(match, file_path, content)
                    api_info["endpoints"].append(endpoint_info)

            except Exception as e:
                print(f"Error analyzing {file_path}: {e}")
                continue

        return api_info

    def _extract_class_info(self, node: ast.ClassDef, file_path: Path, content: str) -> Dict[str, Any]:
        """Extract information about a class"""
        # Get docstring
        docstring = ast.get_docstring(node) or ""

        # Get methods
        methods = []
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append({
                    "name": item.name,
                    "signature": f"{item.name}{ast.unparse(item.args) if hasattr(ast, 'unparse') else '...'}",
                    "docstring": ast.get_docstring(item) or ""
                })

        return {
            "name": node.name,
            "file_path": str(file_path),
            "line_number": node.lineno,
            "docstring": docstring,
            "methods": methods,
            "bases": [ast.unparse(base) if hasattr(ast, 'unparse') else str(base) for base in node.bases]
        }

    def _extract_function_info(self, node: ast.FunctionDef, file_path: Path, content: str) -> Dict[str, Any]:
        """Extract information about a function"""
        # Get signature
        try:
            signature = f"{node.name}({', '.join(arg.arg for arg in node.args.args)})"
        except:
            signature = f"{node.name}(...)"

        # Get docstring
        docstring = ast.get_docstring(node) or ""

        # Parse parameters from docstring (basic parsing)
        parameters = []
        returns = ""
        raises = []

        if docstring:
            # Simple parameter extraction
            lines = docstring.split('\n')
            in_params = False
            for line in lines:
                line = line.strip()
                if line.lower().startswith('args:') or line.lower().startswith('parameters:'):
                    in_params = True
                    continue
                elif line.lower().startswith('returns:') or line.lower().startswith('return:'):
                    in_params = False
                    returns = line.split(':', 1)[1].strip() if ':' in line else ""
                    continue
                elif line.lower().startswith('raises:') or line.lower().startswith('exceptions:'):
                    in_params = False
                    raises = [line.split(':', 1)[1].strip()] if ':' in line else []
                    continue
                elif in_params and line and not line.startswith((' ', '\t')):
                    in_params = False

                if in_params and ': ' in line:
                    param_name, param_desc = line.split(': ', 1)
                    parameters.append({"name": param_name.strip(), "description": param_desc.strip()})

        return {
            "name": node.name,
            "file_path": str(file_path),
            "line_number": node.lineno,
            "signature": signature,
            "docstring": docstring,
            "parameters": parameters,
            "returns": returns,
            "raises": raises
        }

    def _extract_endpoint_info(self, match: re.Match, file_path: Path, content: str) -> Dict[str, Any]:
        """Extract information about an API endpoint"""
        path = match.group(1)

        # Find the function this decorator is attached to
        lines = content.split('\n')
        start_line = match.start()
        line_num = content[:start_line].count('\n') + 1

        # Look for the function definition after this decorator
        func_match = None
        for i in range(line_num - 1, len(lines)):
            line = lines[i].strip()
            func_match = self.doc_patterns['function_def'].search(line)
            if func_match:
                break

        func_name = func_match.group(1) if func_match else "unknown"

        return {
            "path": path,
            "method": "GET",  # Default, would need more sophisticated parsing
            "function_name": func_name,
            "file_path": str(file_path),
            "line_number": line_num
        }

    def _generate_api_docs_content(self, api_info: Dict[str, Any]) -> str:
        """Generate markdown content for API documentation"""
        lines = []
        lines.append("# API Documentation")
        lines.append("")
        lines.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        # Endpoints section
        if api_info["endpoints"]:
            lines.append("## API Endpoints")
            lines.append("")
            for endpoint in api_info["endpoints"]:
                lines.append(f"- `{endpoint['method']} {endpoint['path']}` - {endpoint['function_name']} ({endpoint['file_path']})")
            lines.append("")

        # Classes section
        if api_info["classes"]:
            lines.append("## Classes")
            lines.append("")
            for cls in api_info["classes"]:
                lines.append(self.templates["python_class"].format(
                    class_name=cls["name"],
                    file_path=cls["file_path"],
                    line_number=cls["line_number"],
                    description=cls["docstring"].split('\n')[0] if cls["docstring"] else "No description",
                    inheritance=", ".join(cls["bases"]) if cls["bases"] else "None",
                    methods="\n".join([f"- `{m['name']}` - {m['docstring'].split('\n')[0] if m['docstring'] else 'No description'}" for m in cls["methods"]]),
                    usage_example=f"from {cls['file_path'].replace('/', '.').replace('.py', '')} import {cls['name']}\n\ninstance = {cls['name']}()"
                ))
                lines.append("")

        # Functions section
        if api_info["functions"]:
            lines.append("## Functions")
            lines.append("")
            for func in api_info["functions"]:
                lines.append(self.templates["python_function"].format(
                    function_name=func["name"],
                    file_path=func["file_path"],
                    line_number=func["line_number"],
                    signature=func["signature"],
                    description=func["docstring"].split('\n')[0] if func["docstring"] else "No description",
                    parameters="\n".join([f"- `{p['name']}`: {p['description']}" for p in func["parameters"]]) if func["parameters"] else "None",
                    returns=func["returns"] or "None",
                    raises=", ".join(func["raises"]) if func["raises"] else "None"
                ))
                lines.append("")

        return "\n".join(lines)

    async def _gather_project_info(self) -> Dict[str, Any]:
        """Gather information about the current project"""
        # Read package info
        pyproject_path = self.workspace_path / "pyproject.toml"
        setup_path = self.workspace_path / "setup.py"
        package_json_path = self.workspace_path / "package.json"

        project_name = "Santiago Project"
        description = "AI-powered software development automation platform"

        if pyproject_path.exists():
            # Parse pyproject.toml (simplified)
            content = pyproject_path.read_text()
            if 'name = "' in content:
                name_match = re.search(r'name\s*=\s*"([^"]+)"', content)
                if name_match:
                    project_name = name_match.group(1)
            if 'description = "' in content:
                desc_match = re.search(r'description\s*=\s*"([^"]+)"', content)
                if desc_match:
                    description = desc_match.group(1)

        # Get documentation links
        docs_path = self.workspace_path / "docs"
        doc_links = []
        if docs_path.exists():
            for md_file in docs_path.rglob("*.md"):
                rel_path = md_file.relative_to(self.workspace_path)
                title = md_file.stem.replace('_', ' ').title()
                doc_links.append(f"- [{title}]({rel_path})")

        return {
            "project_name": project_name,
            "description": description,
            "quick_start": "See documentation for setup instructions",
            "doc_links": "\n".join(doc_links) if doc_links else "No documentation found",
            "dev_setup": "See CONTRIBUTING.md for development setup",
            "status_badges": "![Build Status](https://img.shields.io/badge/build-passing-brightgreen)",
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    async def _scan_documentation_issues(self, source_paths: List[Path]) -> List[Dict[str, Any]]:
        """Scan codebase for documentation issues"""
        issues = []

        for source_path in source_paths:
            python_files = list(source_path.rglob("*.py"))

            for file_path in python_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # Check for missing docstrings
                    tree = ast.parse(content)
                    for node in ast.walk(tree):
                        if isinstance(node, (ast.ClassDef, ast.FunctionDef)):
                            if not ast.get_docstring(node):
                                issues.append({
                                    "type": "missing_docstring",
                                    "severity": "medium",
                                    "file": str(file_path),
                                    "line": node.lineno,
                                    "element": node.name,
                                    "element_type": "class" if isinstance(node, ast.ClassDef) else "function",
                                    "suggestion": f"Add docstring to {node.name}"
                                })

                    # Check for TODO/FIXME comments
                    for match in self.doc_patterns['todo_comment'].finditer(content):
                        issues.append({
                            "type": "todo_comment",
                            "severity": "low",
                            "file": str(file_path),
                            "line": content[:match.start()].count('\n') + 1,
                            "content": match.group(1),
                            "suggestion": "Address TODO comment or convert to issue"
                        })

                except Exception as e:
                    issues.append({
                        "type": "parse_error",
                        "severity": "high",
                        "file": str(file_path),
                        "error": str(e),
                        "suggestion": "Fix syntax errors to enable documentation analysis"
                    })

        return issues

    def _generate_audit_report(self, issues: List[Dict[str, Any]]) -> str:
        """Generate documentation audit report"""
        lines = []
        lines.append("# Documentation Audit Report")
        lines.append("")
        lines.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        # Summary
        severity_counts = {}
        for issue in issues:
            severity_counts[issue["severity"]] = severity_counts.get(issue["severity"], 0) + 1

        lines.append("## Summary")
        lines.append("")
        lines.append(f"- **Total Issues:** {len(issues)}")
        for severity, count in severity_counts.items():
            lines.append(f"- **{severity.title()}:** {count}")
        lines.append("")

        # Issues by type
        type_counts = {}
        for issue in issues:
            type_counts[issue["type"]] = type_counts.get(issue["type"], 0) + 1

        lines.append("## Issues by Type")
        lines.append("")
        for issue_type, count in sorted(type_counts.items()):
            lines.append(f"- **{issue_type.replace('_', ' ').title()}:** {count}")
        lines.append("")

        # Detailed issues
        lines.append("## Detailed Issues")
        lines.append("")

        for issue in issues[:50]:  # Limit to first 50 issues
            lines.append(f"### {issue['type'].replace('_', ' ').title()}")
            lines.append(f"- **File:** {issue['file']}")
            lines.append(f"- **Line:** {issue.get('line', 'N/A')}")
            lines.append(f"- **Severity:** {issue['severity']}")
            lines.append(f"- **Suggestion:** {issue['suggestion']}")
            if 'element' in issue:
                lines.append(f"- **Element:** {issue['element']} ({issue.get('element_type', 'unknown')})")
            lines.append("")

        return "\n".join(lines)

    async def _generate_feature_stub(self, board_id: str, card_id: str) -> Dict[str, Any]:
        """Generate initial documentation stub for a feature"""
        # Get card information from kanban
        # This would integrate with the kanban service
        return {"action": "feature_stub_generated", "card_id": card_id}

    async def _generate_complete_feature_docs(self, board_id: str, card_id: str) -> Dict[str, Any]:
        """Generate comprehensive documentation for completed feature"""
        return {"action": "feature_docs_completed", "card_id": card_id}

    async def _update_api_docs_on_review(self, board_id: str, card_id: str) -> Dict[str, Any]:
        """Update API docs during code review"""
        return {"action": "api_docs_updated", "card_id": card_id}

    async def _update_card_docs_status(self, board_id: str, card_id: str, actions: List[Dict[str, Any]]):
        """Update kanban card with documentation status"""
        # This would add comments/tags to the kanban card
        pass

    async def _analyze_feature_implementation(self, implementation_files: List[Path]) -> Dict[str, Any]:
        """Analyze feature implementation files"""
        analysis = {
            "sections": [],
            "dependencies": [],
            "test_coverage": 0,
            "complexity": "medium"
        }

        for file_path in implementation_files:
            if file_path.exists():
                # Basic analysis
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                analysis["sections"].append({
                    "file": str(file_path),
                    "lines": len(content.split('\n')),
                    "functions": len(self.doc_patterns['function_def'].findall(content)),
                    "classes": len(self.doc_patterns['class_def'].findall(content))
                })

        return analysis

    def _generate_feature_docs_content(self, feature_id: str, analysis: Dict[str, Any]) -> str:
        """Generate feature documentation content"""
        lines = []
        lines.append(f"# Feature: {feature_id}")
        lines.append("")
        lines.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        lines.append("## Implementation Overview")
        lines.append("")
        for section in analysis.get("sections", []):
            lines.append(f"- **{section['file']}**: {section['lines']} lines, {section['functions']} functions, {section['classes']} classes")
        lines.append("")

        lines.append("## Files Changed")
        lines.append("")
        for section in analysis.get("sections", []):
            lines.append(f"- `{section['file']}`")
        lines.append("")

        return "\n".join(lines)

    async def _validate_doc_links(self, doc_files: List[Path]) -> Dict[str, Any]:
        """Validate links in documentation files"""
        results = {
            "valid_links": 0,
            "broken_links": []
        }

        for doc_file in doc_files:
            try:
                content = doc_file.read_text()

                # Find markdown links
                link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
                for match in link_pattern.finditer(content):
                    link_text, link_url = match.groups()

                    # Check if it's a relative link
                    if not link_url.startswith(('http://', 'https://', 'mailto:')):
                        # Convert to absolute path
                        abs_path = (doc_file.parent / link_url).resolve()

                        if not abs_path.exists():
                            results["broken_links"].append({
                                "file": str(doc_file),
                                "link_text": link_text,
                                "link_url": link_url,
                                "absolute_path": str(abs_path)
                            })
                        else:
                            results["valid_links"] += 1

            except Exception as e:
                results["broken_links"].append({
                    "file": str(doc_file),
                    "error": str(e)
                })

        return results

    async def _fix_broken_links(self, broken_links: List[Dict[str, Any]]) -> int:
        """Attempt to fix broken links"""
        fixes_applied = 0

        # Group by file
        files_to_fix = {}
        for link in broken_links:
            file_path = link["file"]
            if file_path not in files_to_fix:
                files_to_fix[file_path] = []
            files_to_fix[file_path].append(link)

        # Process each file
        for file_path, links in files_to_fix.items():
            try:
                content = Path(file_path).read_text()

                # Try to fix links (simplified - just remove broken ones)
                for link in links:
                    old_link = f"[{link['link_text']}]({link['link_url']})"
                    new_link = f"{link['link_text']}"
                    content = content.replace(old_link, new_link)
                    fixes_applied += 1

                Path(file_path).write_text(content)

            except Exception:
                continue

        return fixes_applied

    async def _analyze_code_changes(self, changed_files: List[Path]) -> Dict[str, Any]:
        """Analyze what changed in the code"""
        changes = {
            "new_functions": [],
            "modified_functions": [],
            "new_classes": [],
            "api_changes": []
        }

        # This would do git diff analysis
        # For now, just return basic info
        return changes

    async def _update_docs_for_changes(self, changes: Dict[str, Any], docs_path: Path) -> List[Dict[str, Any]]:
        """Update documentation based on code changes"""
        updates = []

        # This would update relevant documentation files
        # For now, just return empty list
        return updates

    async def _calculate_docs_quality_metrics(self, docs_path: Path, source_path: Path) -> Dict[str, Any]:
        """Calculate documentation quality metrics"""
        metrics = {
            "doc_files_count": 0,
            "code_files_count": 0,
            "code_files_with_docs": 0,
            "coverage_percentage": 0,
            "overall_quality_score": 0,
            "quality_issues": []
        }

        # Count documentation files
        if docs_path.exists():
            metrics["doc_files_count"] = len(list(docs_path.rglob("*.md"))) + len(list(docs_path.rglob("*.rst")))

        # Count source files and check documentation
        if source_path.exists():
            python_files = list(source_path.rglob("*.py"))
            metrics["code_files_count"] = len(python_files)

            for file_path in python_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    tree = ast.parse(content)
                    has_docstrings = False

                    for node in ast.walk(tree):
                        if isinstance(node, (ast.ClassDef, ast.FunctionDef)):
                            if ast.get_docstring(node):
                                has_docstrings = True
                                break

                    if has_docstrings:
                        metrics["code_files_with_docs"] += 1

                except:
                    continue

        # Calculate coverage
        if metrics["code_files_count"] > 0:
            metrics["coverage_percentage"] = (metrics["code_files_with_docs"] / metrics["code_files_count"]) * 100

        # Calculate quality score (simplified)
        metrics["overall_quality_score"] = min(100, metrics["coverage_percentage"] * 0.7 + (metrics["doc_files_count"] * 5))

        return metrics


# Service entry point
async def main():
    """Main entry point for the documentation automation service"""
    import sys
    workspace_path = Path(sys.argv[1]) if len(sys.argv) > 1 else Path.cwd()

    service = DocumentationAutomationService(workspace_path)
    await service.start()


if __name__ == "__main__":
    asyncio.run(main())